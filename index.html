<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Jcoz by Decave</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-dark.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>
      <div id="header">
        <nav>
          <li class="fork"><a href="https://github.com/Decave/JCoz">View On GitHub</a></li>
          <li class="downloads"><a href="https://github.com/Decave/JCoz/zipball/master">ZIP</a></li>
          <li class="downloads"><a href="https://github.com/Decave/JCoz/tarball/master">TAR</a></li>
          <li class="title">DOWNLOADS</li>
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <h1>Jcoz</h1>
          <p>Java Causal Profiler</p>
          <p>By Matthew Perron (mperron) and David Vernet (dcv)</p>
          <hr>
          <span class="credits left">&copy; Copyright 2016 <a href="https://github.com/Decave">David Vernet</a> and <a href="https://github.com/MattPerron">Matthew Perron</a> all rights reserved</span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </div>

    <h2>Summary</h2>
          <p>
              We implemented a causal Java profiler, and were able to use it to optimize the <a href="http://www.h2database.com/html/main.html">Java H2 Database</a>; a widely used, mature in-memory Java database engine.
              Our profiler is also very lightweight, with roughly 10-40% runtime overhead (matching major sampling profilers such as HPROF and JVisualVM).
          </p>

    <h2>Background</h2>
          <p>
              Optimizing parallel programs is hard...very hard. As we've learned in this class, there are many moving parts to consider. For example, if threads are write sharing a global variable
              it can cause excessive interconnect traffic and cache misses. Or perhaps Ahmdal's law prevents us from reaching our performance goal before we even get started. Even worse,
              it could happen that speeding up certain parts of your code might even cause a performance <i>hit</i> (for example, when speeding up a line of code causes increased
              lock contention). Is it possible to handle all of this complexity in a profiler, and provide accurate results so programmers know exactly where to look to optimize
              their parallel programs? It is -- with causal profiling.
          </p>
          <p>
              For our project, we implemented a causal Java profiler; that is, a tool for profiling multithreaded Java programs. A causal profiler detects how changing a line would
              affect performance using something called <i>virtual speedup</i>. At some frequency throughout the runtime of the program, we run an "experiment" in which we
              choose a line being executed among all threads (randomly), and a speedup amount
              between 0 - 100%, to measure how speeding up that line of code by the given speedup percent would affect overall program runtime.
              Ideally, during this experiment, when any thread enters the selected line, all other threads are suspended for a period of time depending
              on the speedup chosen for that experiment. However, we instead utilize sampling to avoid the excessive runtime overhead from having every
              thread pause every time it hits the line. To actually measure line speedup, we record the throughput achieved during the experiment and use it
              to determine how speeding up the line would affect throughput / runtime. Thus, by freezing the other threads and measuring how throughput changes, we have "virtually" sped
              the line being executed by the given thread.
          </p>
          <p>
              The following caption is a useful visualization of virtual speedup (caption credited to Charlie Curtsinger and Emery Berger whose paper is cited in the references section): <br /> <br />
              <img src="http://davidvernet.com/media/virtual_speedup_picture.png" />
          </p>
          <p>
              The above approach to profiling multithreaded programs has many benefits over the traditional performance monitoring profilers.
              For example, observe the following toy program:
              <div class="highlight">
                  <pre>
public class Test {
    static class ThreadTest1 implements Runnable {
        public void run() {
            long sum = 0;
            for( long i = 0; i < 1600000000L; i++ ) {
                sum += 1;
            }
                  
            System.out.println("Thread1 sum: " + sum);
        }
    }
                  
    static class ThreadTest2 implements Runnable {
        public void run() {
            long sum = 0;
            for( long i = 0; i < 1200000000L; i++ ) {
                sum += 1;
            }

            System.out.println("Thread2 sum: " + sum);
        }
    }
                  
    public static void main(String[] args) throws InterruptedException {
        Thread longer = new Thread(new ThreadTest1());
        Thread shorter = new Thread(new ThreadTest2());
        longer.start();
        shorter.start();
        shorter.join(); longer.join();
    }
}
</pre>
              </div>
              For this program, HPROF's full instrumentation profiler gives us the following output:<br /><br />
              <div class="highlight">
                  <pre>
CPU TIME (ms) BEGIN (total = 1486) Fri May  6 22:12:27 2016
rank   self  accum   count trace method
1 35.40% 35.40%       2 302001 java.lang.Thread.join
2 35.33% 70.73%       1 302000 test.Test$ThreadTest1.run
3 25.64% 96.37%       1 301970 test.Test$ThreadTest2.run
4  0.13% 96.50%       1 300163 java.lang.invoke.MethodHandle.&lt;clinit&gt;
5  0.13% 96.64%      10 300845 sun.misc.URLClassPath$JarLoader.ensureOpen
6  0.07% 96.70%       7 300014 java.lang.System.getSecurityManager
7  0.07% 96.77%       1 300094 java.lang.invoke.MethodHandleImpl.&lt;clinit&gt;
8 ..........
</pre>
              </div>

              As you can see, HPROF tells us that we spend over 1/3 of our time in a join statement and in <tt>a()</tt>,
              and 1/4 of our time in <tt>b()</tt>. In reality, optimizing <tt>b()</tt> would have no affect whatsoever on program runtime,
              and optimizing the join statement is meaningless. We see similar results when running HPROF's sampling profiler:<br /><br />

              <div class="highlight">
                  <pre>
CPU SAMPLES BEGIN (total = 78) Fri May  6 22:16:59 2016
rank   self  accum   count trace method
1 55.13% 55.13%      43 300062 test.Test$ThreadTest1.run
2 42.31% 97.44%      33 300063 test.Test$ThreadTest2.run
3  1.28% 98.72%       1 300035 java.util.Arrays.copyOf
4  1.28% 100.00%       1 300061 java.lang.System.arraycopy
</pre>
              </div>

                  Though this profile provides a more accurate picture of where we're spending our time,
                  it still does not indicate that optimizing <tt>b()</tt> does nothing for runtime and that
                  after we speed up <tt>a()</tt> by 25% that the program runtime will top off when <tt>b()</tt>
                  becomes the bottleneck.
              </p>

              <p>
              Now, observe the output of JCoz on the same program:

                <div class="block" style="margin-bottom: 15px;"><img src="http://davidvernet.com/media/25percent_speedup_profile.png" /></div>
                <div class="block" style="margin-bottom: 15px;"><img src="http://davidvernet.com/media/25percent_no_speedup_profile.png" /></div>

                Though there is some variance (as is expected with a Java program), our profile correctly indicates that speeding up
                Thread 1 (the longer thread) linearly approaches a 25% speedup, and then at roughly a 25% line speedup, the overall runtime
                speedup stays around 25% no matter how much faster we make the line (because thread 2 dominates runtime). Additionally,
                we see that the profile indicates that optimizing Thread 2 does essentially nothing for performance.
                We will demonstrate more results in the results section below.
              </p>

              <h2>Approach</h2>

              <h3>Profiler Workflow</h3>
              <p>
                  Our profiler follows this workflow when profiling a program:
                  <ol>
                      <li>Start the program, and set a breakpoint (a "progress point") at a line in the program that was chosen by the user to measure throughput</li>
                      <li>Wait for a given warmup period to avoid overhead during the initial part of a program where the progress point will not be hit</li>
                      <li>Starting running experiments. An experiment includes the following steps:
                            <ol>
                                <li>Choose a line for speedup randomly among the currently executing threads, as well as a random speedup between 0 - 100%</li>
                                <li>
                                    Every 1ms, send a SIGPROF signal to all user threads, in which they check whether they are on the experiment line. If so,
                                    all other threads are frozen for a period of time dependent on the speedup chosen for this experiment, and the given thread continues
                                    to execute
                                </li>
                                <li>
                                    Throughout the runtime of the experiment, keep track of how many times a given "progress-point" line is hit by all threads. This
                                    progress point line is how we measure how throughput changes when we virtually speed up a line
                                </li>
                                <li>After a certain amount of time, end the experiment and write the metrics to a file buffer</li>
                            </ol>
                      </li>
                      <li>When the program has finished running, flush the file buffer</li>
                  </ol>
                  A program can be run more than once -- the results are simply appended to the given profile output.
              </p>

              <h3>Platform</h3>
              <p>
                  Our profiler is written in C++ as a Java Agent, and targets Linux machines with a JDK with Java >= 1.7 installed.
                  Other than that, our project has no dependencies other than the standard C and C++ libraries. To interpret
                  a profile output from our profiler, we used the <a href="http://plasma-umass.github.io/coz/">original COZ profile plotter</a>.
              </p>

             <h3>Implementation</h3>
              <p>
                  In building our tool, we leveraged the <a href="https://docs.oracle.com/javase/8/docs/platform/jvmti/jvmti.html">JVM Tool Interface (JVMTI)</a>
                  to get thread stack traces, listen for when new user threads are created and existing threads are killed, map byte code instructions to source file
                  line numbers, and set breakpoints. Though much of the interface was relatively straightforward and easy to use, we ran into a hurdle in obtaining stack 
                  traces in real time from the program. JVMTI only publicly exposes a <tt>GetStackTrace</tt> function that returns stack
                  traces at "safe points". Had we been constrained to use this "safe" thread stack function, it would have severely degraded the utility of our 
                  profiler because it may have skipped important lines to profile, or caused us to wait for some indefinite amount of time before being given the
                  stack trace (which would have caused our sampling logic to be completely unpredictable). For example, we observed that no safe points were
                  being added in tight, expensive loops. To get around this issue, we used the undocumented
                  <tt>AsyncGetStackTrace</tt> function to get stack traces in non-safe points. It took quite a lot of scaffolding and patchy code to properly use
                  this function, and we ended up integrating our code with the <a href="https://github.com/dcapwell/lightweight-java-profiler">lightweight-java-profiler</a>
                  codebase, as they had previously added some complicated code to use the <tt>AsyncGetStackTrace</tt> function.
              </p>
              <p>
                  Another technique we used in our profiler was sampling. It would have required far too much overhead to listen to each instruction
                  for every thread to see if a thread was executing an experiment line. Instead, we send all threads a <tt>SIGPROF</tt> signal every 1 ms. In the signal
                  handler, each thread determines if it is executing the experiment line by calling <tt>AsyncGetStackTrace</tt>. If so, it notifies other threads to sleep 
                  by incrementing a global sleep counter. This provides a nice tuning knob for configuring the profiler, as we can easily adjust
                  experiment duration and sampling frequency to tune the runtime overhead and profiling granularity of JCoz.
              </p>
              <p>
                  We occassionally referenced the <a href="https://github.com/plasma-umass/coz">original COZ implementation</a> 
                  to gain insights into how they implemented certain techniques. In addition, as mentioned above, we integrated parts of our code with the
                  <a href="https://github.com/dcapwell/lightweight-java-profiler">lightweight-java-profiler</a> to take advantage of their approach to
                  using the undocumented JVMTI method <tt>AsyncGetCallTrace</tt>.
              </p>

            <h2>Results</h2>
              <p>
                  We chose to measure performance with three different metrics:
                  <ol>
                    <li>
                        Does our profiler gave us accurate results for toy programs that exhibited
                        various examples of parallel programming (i.e. the example given above, threads thrashing on locks, Ahmald's law with serial execution, etc)?
                    </li>
                    <li>
                        How much overhead is incurred from using our profiler?
                    </li>
                    <li>
                        Can we use the profiler to find optimizations in complex, mature, and widely used libraries?
                    </li>
                  </ol>
                  We were successful in all three of the above metrics, and cover each in more detail below.
              </p>

              <p>
                  Our first metric of success was our profiler's performance on simple Java programs that exhibited various paradigms of parallel programming. 
                  We were able to measure this by running our profiler on many different, easy to understand toy examples, 
                  and verifying that the profiler gave us the ouput we expected for all programs. We found that all examples were correctly profiled, though in some instances, many iterations of the program was required to build a large
                  enough profile to have useful results. We ran our profiler on the following types of toy examples:
                  <ol>
                      <li>Two non-contending threads with different runtimes (shown above)</li>
                      <li>Multiple non-contending threads with exactly the same runtimes (equal speedup opportunities in all threads)</li>
                      <li>Two threads with the same runtimes where one is waiting on a lock (essentially serial execution -- equal speedup opportunities found)</li>
                      <li>Multiple threads thrashing on a single lock (significant speedup found for the small section of code where a thread held the lock)</li>
                  </ol>

                  We found that the profile outputs matched our expectations for all of the
                  aforementioned examples. Going forward, we intend to manufacture more toy examples; including examples with threads that exhibit poor cache locality,
                  threads that write to a global atomic variable, and threads that flood the interconnect with excessive traffic. We are confident that our
                  profiler can at least correctly identify poor cache locality, as we were able to identify a bottleneck in the Universal Java Matrix Package (UJMP)
                  that exhibited very poor cache locality when multiplying dense matrices (though we did not have time to implement an optimization).
              </p>

              <p>
                  Perhaps most importantly, we were able to optimize real libraries with our profiler. Specifically, we were able to optimize 
                  the <a href="http://www.h2database.com/html/main.html">Java H2 Database</a> by 19% using the standard <a href="http://dacapobench.org/">Dacapo Benchmark suite</a>,
                  and we also identified an opportunity for optimization on the high performance, highly parallel 
                  <a href="https://github.com/ujmp/universal-java-matrix-package">Universal Java Matrix Package</a> by using blocking to take better advantage of the cache.
              </p>

              <p>
                  JCoz joins a community of existing Java profilers, including 
                  <a href="http://docs.oracle.com/javase/7/docs/technotes/samples/HPROF.html">HPROF</a>,
                  <a href="http://docs.oracle.com/javase/6/docs/technotes/tools/share/jvisualvm.html">JVisualVM</a>,
                  and <a href="https://github.com/dcapwell/lightweight-java-profiler">lightweight java profiler</a>
              </p>
            
              <p>
                  Another metric of success for our profiler was the amount of runtime overhead incurred from using it.
                  We found that our profiler was very lightweight, incurring roughly 10-40% runtime overhead for all programs that we profiled.
                  Below is a comparison of the overhead of running different Java Profilers on the Dacapo H2 Benchmarking tool.
                  <div class="block" style="margin-bottom: 15px;"><img src="http://davidvernet.com/media/profiling_overhead.png" /></div>
                  Since JCoz does not use the heavyweight instrumentation that is used by the JVisualVM profiling tool, its overhead is similar to HPROF and JVisualVM profiling. 
              </p>

              <p>
                  One of our main goals for this project was to take an application of which we had little knowledge, run our profiler, and make a meaningful improvment with little development time.
                  We chose to profile the H2 TPCC benchmark using the Dacapo Java Benchmarking Suite. Our profile displayed the line as the best opportunity for 
                  throughput improvement with the following graph. We ran on muir, a machine with 264GB of DRAM and 4 Intel Intel Xeon E5-4650 2.70 GHz processors.
                  <div class="block" style="margin-bottom: 15px;"><img src="http://davidvernet.com/media/h2_optimized_line_profile_output.png" /></div>
              </p>

              <p>
                  Using the above profile, we discovered that a <tt>Thread.Wait()</tt> call meant to implement backoff of a failed 
                  transaction before retrying was causing an significant drop in throughput performance. 
                  By reducing the wait time from a random time between 1 and 10 ms to 1-10 microseconds. 
                  Changing only this line we observed a throughput increase of about 19%. We had no knowledge of this codebase before running our profiler, 
                  and this allowed us to improve the throughput of the benchmark in about an hour after viewing the profile results.
              </p>
              <p>
                  Here is the line we identified for speedup:
                  <div class="highlight">
                      <pre>
Database database = session.getDatabase();
int sleep = 1 + MathUtils.randomInt(10);
while (true) {
    try {
        if (database.isMultiThreaded()) {
            Thread.sleep(sleep); // **This line was identified as a bottleneck**
        } else {
            database.wait(sleep);
        }
    } catch (InterruptedException e1) {
        // ignore
    }
    long slept = System.nanoTime() / 1000000 - now;
    if (slept >= sleep) {
        break;
    }
}
</pre>
                  </div>
              </p>
          <p>
              To speed up the line of code, we simply lowered the amount of time to randomly sleep on a failed transaction:
              <div class="highlight">
                  <pre>
Database database = session.getDatabase();

// now sleeping between 1 - 10 microseconds rather than 1 - 10 milliseconds
int sleep = 1000 * (1 + MathUtils.randomInt(10));
while (true) {
    try {
        if (database.isMultiThreaded()) {
            Thread.sleep(0, sleep);
        } else {
            database.wait(0, sleep);
        }
    } catch (InterruptedException e1) {
        // ignore
    }
    long slept = System.nanoTime() - now;
    if (slept >= sleep) {
        break;
    }
}
</pre>
              </div>
          </p>

              <p>
                  The results of the end to end runtime of the benchmark before and after optimization are displayed below.
                  <div class="block" style="margin-bottom: 15px;"><img src="http://davidvernet.com/media/TPCC_before_and_after.png" /></div>
              </p>

              <p>
                  We feel this significant improvment with little development effort demonstrates the utility of this form of profiling. 
              </p>              

          <h2>References</h2>
              <p>
                  <ol>
                      <li>C. Curtsinger, E. Berger. COZ: Finding Code that Counts with Causal Profiling. <i>SOSP '15 ACM SIGOPS</i></li>
                      <li>J. Manson, D. Capwell. <a href="https://github.com/dcapwell/lightweight-java-profiler">Lightweight java profiler</a></li>
                  </ol>
              </p>

          <h3>Work allocation</h3>
            <p>
                Equal work was done by both partners, with slightly more work being by David towards the end of the project because he had more time 
                (Matt had a large database projects and other finals).
            </p>
      </section>
  </body>
</html>
